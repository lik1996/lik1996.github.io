
<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Kun Li (Petrick)</title>
	<meta content="Kun Li, lik1996.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 14pt;
}
b.paper {
  font-weight: bold;
  font-size: 14pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1050px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 17pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight:bold;
}
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
	font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 230px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}
span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-45959174-3', 'kailigo.github.io');
  ga('send', 'pageview');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="KunLi" style="float: left; padding-left: .01em; height: 140px;" src="KunLi.jpg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Kun Li</span><br />
<span><strong>Ph.D. candidate</strong></span><br />
<span><a href='https://sites.google.com/site/michaelyingyang/home'>Scene Understanding Group</a>, <a href='https://www.itc.nl/'>ITC</a>, <a href='https://www.utwente.nl/en/'>University Of Twente</a></span><br />
<span><strong>Location</strong>: Enschede, Netherlands</span><br />
<span><strong>Email</strong>: k.li@utwente.nl         || <a href='https://scholar.google.com/citations?user=wVZVqfEAAAAJ&hl=en'>Google Scholar</a> || <a href='https://www.researchgate.net/profile/Kun-Li-168'>ResearchGate</a> || <a href='https://orcid.org/0000-0001-8501-3916'>ORCID</a> </span><br />
<!-- <span> || <a href='https://www.researchgate.net/profile/Kun-Li-168'>LinkedIn</a> GitHub  &nbsp &nbsp -->
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>About Me</h2>
<div class="paper">
I am a Ph.D. candidate in EOS Department of ITC Faculty, University of Twente, supervised by <a href='https://research.utwente.nl/en/persons/george-vosselman'>Prof. George Vosselman</a> and <a href='https://sites.google.com/site/michaelyingyang'>Prof. Michael Ying Yang</a> (University Of Bath, UK).
<!-- Prior to this, I obtained my master (2018-2021, supervised by <a href='http://jszy.whu.edu.cn/huxiangyun/zh_CN/index.htm'>Prof. Xiangyun Hu</a>) and bachelor (2014-2018, supervised by <a href='http://jszy.whu.edu.cn/zzchen'>Prof. Zhenzhong Chen</a>) degrees at <a href='http://www.whu.edu.cn/'>Wuhan University</a>. -->
My Ph.D. research interests lie in Interactive Vision-Language Learning and Image Processing via deep learning-based techniques.<p>

<br> Since April 2025, I have been a guest researcher at the IT University of Copenhagen in Denmark, working on multimodal learning and audio2video generation in collaboration with the Audio–Visual Computing Research Group, led by <a href='https://pure.itu.dk/en/persons/sami-brandt'>Prof. Sami Brandt</a>. 
<p> 
	
<br> <strong>I am looking for a postdoctoral position. If there are relevant opportunities in my field, please feel free to contact me.</strong>
<!-- <br> Feel free to contact me if you are interested in similar topics. -->

</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2>Research Interests</h2>
<div class="paper">
Vision and language learning, Large language models, Multimodal learning, Image segmentation, Object detection, Audio2video generation, Remote sensing, Satellite imagery interpretation, Change detection.
<p>

</div>
</div>
</div>
	
<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
  <li> 2025.05: One paper about referring image segmentation accepted by ISPRS P&RS.</li>
  <li> 2025.04: One paper about explainable VQA accepted by CVPRW2025 in Nashville, USA.</li>
  <li> 2025.04: Visit Copenhagen as a guest researcher at the IT University of Copenhagen in Denmark.</li> 
  <li> 2024.06: One paper about aerial image VQA benchmark accepted by ISPRS P&RS.</li>
  <li> 2024.03: One co-author paper about multimodal change detection accepted by INFFUS.</li>
  <li> 2023.10: One paper about interactive image segmentation accepted by ICCVW2023 in Paris, France.</li>
<!--   <li> 2023.07: Attend the 13th Lisbon Machine Learning Summer School organized by Instituto Superior Técnico in Lisbon, Portugal.</li> -->
<!--   <li> 2023.07: Oral presentation on Netherlands Center for Geodesy and Geo-Informatics (NCG) Symposium in Enschede, Netherlands.</li>
  <li> 2023.01: PhD's spotlight on Meeting on Development And Sharing of Open Geodata in Enschede, Netherlands.</li> -->
  <li> 2022.03: Pass the public Ph.D. Qualifier with committee: Prof. George Vosselman, Dr. Michael Ying Yang, Dr. Sylvain Lobry (Université de Paris, France).</li>  
  <li> 2021.09: Begin my new Ph.D. journey at the University of Twente in the Netherlands.</li>   
  <li> 2021.07: Funded by China Scholarship Council (CSC) for 4 years.</li>   
    </ul>
  </div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Educations</h2>
  <div class="paper">
    <ul>
  <li> 2021.09-2025.08(scheduled): Ph.D. in Faculty of Geo-Information Science and Earth Observation, University Of Twente, Netherlands.</li>
  <li> 2018.09-2021.06: M.Sc. in School of Remote Sensing and Information Engineering, Wuhan University, China.</li>  
  <li> 2014.09-2018.06: B.Sc. in School of Remote Sensing and Information Engineering, Wuhan University, China.</li>   
    </ul>
  </div>
</div>
</div>

<!-- <div style="clear: both;">
<div class="section">
  <h2>Work Experiences</h2>
  <div class="paper">
    <ul>
  <li> 2024.11-2025.03: Intership at B.Sc. in School of Remote Sensing and Information Engineering, Wuhan University, China.</li>   
    </ul>
  </div>
</div>
</div> -->

<div style="clear: both;">
<div class="section">
  <h2>Selected Peer-reviewed Publications
  </h2>
  <div class="paper">


<ul>
<li><p> Scale-wise Bidirectional Alignment Network for Referring Remote Sensing Image Segmentation. [<a href='https://arxiv.org/abs/2501.00851'>PDF</a>]
<br>
<u>Kun Li</u>, George Vosselman, Michael Ying Yang.
<br>
<i>
ISPRS Journal of Photogrammetry and Remote Sensing (<strong>ISPRS P&RS</strong>), 2025.
</i>
</p>
</li>
</ul>
	  
<ul>
<li><p> Multimodal Rationales for Explainable Visual Question Answering. [<a href='https://arxiv.org/abs/2402.03896'>PDF</a>]
<br>
<u>Kun Li</u>, George Vosselman, Michael Ying Yang.
<br>
<i>
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (<strong>CVPRW</strong>), 2025.
</i>
</p>
</li>
</ul>

<ul>
<li><p> HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images. [<a href='https://www.sciencedirect.com/science/article/pii/S0924271624002326'>PDF</a>]
<br>
<u>Kun Li</u>, George Vosselman, Michael Ying Yang.
<br>
<i>
ISPRS Journal of Photogrammetry and Remote Sensing (<strong>ISPRS P&RS</strong>), 2024.
</i>
</p>
</li>
</ul>
	  
<ul>
<li><p> Transformer-based Multimodal Change Detection with Multitask Consistency Constraints. [<a href='https://www.sciencedirect.com/science/article/pii/S1566253524001362?via%3Dihub'>PDF</a>]
<br>
Biyuan Liu, Huaixin Chen, <u>Kun Li</u>, Michael Ying Yang.
<br>
<i>
Information Fusion (<strong>INFFUS</strong>), 2024.
</i>
</p>
</li>
</ul>
	  
	  
<ul>
<li><p> Interactive Image Segmentation with Cross-Modality Vision Transformers. [<a href='https://openaccess.thecvf.com/content/ICCV2023W/NIVT/papers/Li_Interactive_Image_Segmentation_with_Cross-Modality_Vision_Transformers_ICCVW_2023_paper.pdf'>PDF</a>]
<br>
<u>Kun Li</u>, George Vosselman, Michael Ying Yang.
<br>
<i>
Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (<strong>ICCVW</strong>), 2023.
</i>
</p>
</li>
</ul>
	  
<ul>
<li><p> A Deep Interactive Framework for Building Extraction in Remotely Sensed Images Via a Coarse-to-Fine Strategy. [<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9553424'>PDF</a>]
<br>
<u>Kun Li</u>, Xiangyun Hu.
<br>
<i>
IEEE International Geoscience and Remote Sensing Symposium (<strong>IGARSS</strong>), 2021.
</i>
</p>
</li>
</ul>

<!-- <ul>
<li><p> A Siamese convolutional neural network with high–low level feature fusion for change detection in remotely sensed images. [<a href='https://www.tandfonline.com/doi/full/10.1080/2150704X.2021.1892851'>PDF</a>]
<br>
Hao Zhou, Mi Zhang, Xiangyun Hu, <u>Kun Li</u>, Jing Sun.
<br>
<i>
Remote Sensing Letters, 2021.
</i>
</p>
</li>
</ul> -->

	  
<ul>
<li><p> Attention-Guided Multi-Scale Segmentation Neural Network for Interactive Extraction of Region Objects from High-Resolution Satellite Imagery. [<a href='https://www.mdpi.com/2072-4292/12/5/789'>PDF</a>]
<br>
<u>Kun Li</u>, Xiangyun Hu, Huiwei Jiang, Zhen Shu, Mi Zhang.
<br>
<i>
Remote Sensing (<strong>RS</strong>), 2020.
</i>
</p>
</li>
</ul>

<!-- <ul>
<li><p> PGA-SiamNet: Pyramid feature-based attention-guided Siamese network for remote sensing orthoimagery building change detection. [<a href='https://www.mdpi.com/2072-4292/12/3/484'>PDF</a>]
<br>
Huiwei Jiang, Xiangyun Hu, <u>Kun Li</u>, Jinming Zhang, Jinqi Gong, Mi Zhang.
<br>
<i>
Remote Sensing (<strong>RS</strong>), 2020.
</i>
</p>
</li>
</ul>		   -->

<!-- <ul>
<li><p> Patch matching and dense CRF-based co-refinement for building change detection from Bi-temporal aerial images. [<a href='https://www.mdpi.com/1424-8220/19/7/1557'>PDF</a>]
<br>
Jinqi Gong, Xiangyun Hu, Shiyan Pang, <u>Kun Li</u>.
<br>
<i>
Sensors, 2019.
</i>
</p>
</li>
</ul>		   -->

	  
</div>
</div>
</div>

	
<div style="clear: both;">
<div class="section">
  <h2>Preprints
  </h2>
  <div class="paper">

<ul>
<li><p> Learning from Exemplars for Interactive Image Segmentation. [<a href='https://arxiv.org/abs/2406.11472'>PDF</a>]
<br>
<u>Kun Li</u>, Hao Cheng, George Vosselman, Michael Ying Yang.
<br>
<i>
arXiv, 2024 (Under review).
</i>
</p>
</li>
</ul>

	
<!-- <ul>
<li><p> Convincing Rationales for Visual Question Answering Reasoning. [<a href='https://arxiv.org/abs/2402.03896'>PDF</a>]
<br>
<u>Kun Li</u>, George Vosselman, Michael Ying Yang.
<br>
<i>
arXiv, 2024 (accepted).
</i>
</p>
</li>
</ul> -->


<!-- <ul>
<li><p> HRVQA: A Visual Question Answering Benchmark for High-Resolution Aerial Images. [<a href='https://arxiv.org/abs/2301.09460'>PDF</a>]
<br>
<u>Kun Li</u>, George Vosselman, Michael Ying Yang.
<br>
<i>
arXiv, 2023.
</i>
</p>
</li>
</ul> -->

</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
  <h2>Presentations
  </h2>
  <div class="paper">

<!-- <ul>
<li><p> Poster presentation on CVPR 2025 Workshop on Multimodal Learning and Applications. [<a href='https://mula-workshop.github.io/'>Link</a>]
<br>
Nashville, USA, 2025.06.
<br>
<i>
</i>
</p>
</li>
</ul>	   -->

<ul>
<li><p> Poster presentation on ICCV 2023 Workshop on New Ideas in Vision Transformers. [<a href='https://sites.google.com/view/nivt-iccv2023/recording#h.x3im9rwzawtl'>Link</a>]
<br>
Paris, France, 2023.10.
<br>
<i>
</i>
</p>
</li>
</ul>

<ul>
<li><p> Attender's spotlight on the 13th Lisbon Machine Learning Summer School (LxMLS 2023). [<a href='http://lxmls.it.pt/2023/index.html'>Link</a>]
<br>
Lisbon, Portugal, 2023.07.
<br>
<i>
</i>
</p>
</li>
</ul>
	  
<ul>
<li><p> Oral presentation on Netherlands Center for Geodesy and Geo-Informatics (NCG) Symposium. [<a href='https://www.ncgeo.nl/index.php/en/actueelgb/nieuwsgb/item/2870-ncg-symposium-2023'>Link</a>]
<br>
Enschede, Netherlands, 2023.07.
<br>
<i>
</i>
</p>
</li>
</ul>


<ul>
<li><p> Ph.D.'s spotlight on Meeting on Development And Sharing of Open Geodata. [<a href='https://www.itc.nl/about-itc/centres-of-expertise/big-geodata/meetings/events/development-and-sharing-of-open-geodata/'>Link</a>]
<br>
Enschede, Netherlands, 2023.01.
<br>
</p>
</li>
</ul>

</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professonal Activities</h2>
<div class="paper">
<ul>
<p><font size="5">
	<li> Top Reviewers for NeurIPS (2023, 2024).</li>
	<li> Reviewer for conferences: CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, AAAI, ICME, ACM MM.</li>    	 	
        <li> Reviewer for journals: IJCV, ISPRS P&RS, TGRS, GRSL. </li>
    <!-- <li> Guest reviewer for AAAI 2017. </li>    	  -->
        <li> IEEE/CVF student member. </li>
	<li> Supervisor for Master Thesis ({Akshay Chaprana, 2023}). </li>
	<li> Teaching Assistant for University of Twente courses ({2D and 3D Scene Analysis, 2021}, {Image Analysis, 2021, 2022}, {AI for Autonomous Robots, 2023}). </li>
</font></p>
</ul>
</div>
</div>
</div>



<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 31st March, 2025</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<!-- <hr> -->
<!-- <!--  -->
<div id="clustrmaps-widget"></div><script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=350&t=m&d=HIPtNS_TzvxFQAggJn4_14PasHDypjb3f7SUS0q3j20'></script>
<!-- -->
</body>
</html>
